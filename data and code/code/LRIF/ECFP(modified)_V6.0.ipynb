{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于ECFP的数据包处理\n",
    "# 最后更新：2020.05.07 戴以恒\n",
    "# 参数及说明：\n",
    "# 输入文件：包含两列，第一列标题为'smiles'，第二列标题任意，为标签值\n",
    "# 输出文件：一共6个：X，y，特征列标题Title，无标题的SMILES，两个整合的数据集\n",
    "INPUT_NAME = \"CuData_94.csv\"\n",
    "RADIUS = 3                                # 整数，设置ECFP提取特征的半径\n",
    "\n",
    "# V1.5加入'特征列填充率保留模块'：\n",
    "FEATURE_RESERVE = 0.011                   # 浮点数，特征矩阵内非零数据的比例大于等于这个一般保留阈值，这个特征就会被保留\n",
    "EXACT_PIECE_RESERVE = True                # 布尔值，表示是否开启使用特定基团保留片段的功能\n",
    "SMILES_PATT = ['CO']                      # 字符串列表，每个元素都是一段SMILES值(其实用的是SMART，这样可以匹配类似'ccc'的芳环残片)，用于在保留时作为匹配模式(仅在EXACT_PIECE_RESERVE为True时生效)\n",
    "EXACT_PIECE_RESERVE_THRESHOLD = 0.011     # 浮点数，针对特定基团的阈值，和上面的类似，若想要全部保留含有某特定基团的特征，此项请用0.0(仅在EXACT_PIECE_RESERVE为True时生效)\n",
    "# 假如只想保留含有特定基团的片段，可以把FEATURE_RESERVE设为1.0，而EXACT_PIECE_RESERVE设为0.0，因为特定基团筛选的优先级高于一般保留阈值的筛选\n",
    "\n",
    "# V2.0为针对'CO'的处理而加入，V5.0修改为更具有普遍性的'布尔值转化模块'：\n",
    "# 作用：将一个max值为n的特征列转化为n或(n+1)列布尔值特征列，分别表示拥有(0),1,2,3,...,n个特征片段\n",
    "# 注意：若进行全部数据的转化以求获得一个只由{0, 1}构成的特征矩阵，不应和加入RDKit描述符的功能一起使用\n",
    "# V5.1：添加了自动删除空特征列的功能\n",
    "# V6.0：整合了两种转化为Bool值的模式，即a版(n列)与b版(n+1列)\n",
    "TURN_TO_BOOL = False                      # 布尔值，作为布尔值转化模块的总开关使用\n",
    "ALL_TURN_TO_BOOL = True                   # 布尔值，若为True，则将所有非{0, 1}整数数据全部转化为多个布尔值特征列，最后的特征值矩阵将完全由{0, 1}构成\n",
    "BOOL_PATT_LIST = []                       # 字符串列表，仅在ALL_TURN_TO_BOOL为False时生效，表示将列表中指定的基团转变为布尔值特征内(注意：必须特征列的标题和这里面的字符串一样才进行转换操作)\n",
    "BOOL_TURN_MODE = 'a'                      # 字符，'a'或'b'，代表两种布尔值转化的模式，a为n列，b为n+1列\n",
    "\n",
    "# V3.0加入'RDKit描述符合并模块'：\n",
    "INCLUDE_DESC = False                      # 布尔值，表示是否需要计算并加入RDKit的描述符\n",
    "\n",
    "# V4.0加入'片段原子数量调控模块'：\n",
    "ATOM_COUNT_CONTROL = True                 # 布尔值，作为“原子数量调控模块”的总开关\n",
    "SMARTS_MIN_LENGTH = 2                     # 整数值，表示特征SMARTS内非氢原子的最小数目，包含非氢原子数目大于等于该数目的基团片段才会被保留，设为0就会跳过这一步骤\n",
    "SMARTS_MAX_LENGTH = 20                    # 整数值，表示特征SMARTS内非氢原子的最小数目，包含非氢原子数目小于等于该数目的基团片段才会被保留，若不想在最大值上设限，可设为5*RADIUS或更大的数值\n",
    "ATOM_COUNT_CONTROL_OMIT_PATTERN = []      # 字符串列表，表示若特征片段内含有此列表中的SMARTS片段，此列将不受“原子数量调控模块”的调控\n",
    "\n",
    "# V4.1加入：新输出一个带Title和Value的完整的数据矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['smiles' 'FEC2+(20%)']\n",
      "Number of examples in dataset: 94\n"
     ]
    }
   ],
   "source": [
    "from deepchem.utils.save import load_from_disk\n",
    "dataset_file= INPUT_NAME\n",
    "dataset = load_from_disk(dataset_file)\n",
    "# 数据集默认第一列是SMILES，第二列是标签值，放入数据前请检查好\n",
    "print(\"Columns of dataset: %s\" % str(dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义‘特征器‘’(Featurizer)来将SMILES转化为某些特征\n",
    "import deepchem as dc\n",
    "# featurizer_fix = dc.feat.CircularFingerprint(size=1024)\n",
    "featurizer_sparse = dc.feat.CircularFingerprint(size=1024, sparse=True, smiles=True, radius=RADIUS)\n",
    "if INCLUDE_DESC:\n",
    "    featurizer_desc = dc.feat.RDKitDescriptors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from CuData_94.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.160 s\n",
      "TIMING: dataset construction took 0.171 s\n",
      "Loading dataset from disk.\n",
      "Sparse:\n",
      "((94,), (94, 1), (94, 1), (94,))\n",
      "(94,)\n"
     ]
    }
   ],
   "source": [
    "# 对SMILES进行特征化，输出为哈希值以及SMILES块\n",
    "# 因为hash值与smiles块一一对应，我们只需挑出unique的SMILES/hash作为特征即可\n",
    "loader_sparse = dc.data.CSVLoader(\n",
    "      tasks=[dataset.columns.values[1]], smiles_field=dataset.columns.values[0],\n",
    "      featurizer=featurizer_sparse)\n",
    "dataset_sparse = loader_sparse.featurize(dataset_file)\n",
    "print('Sparse:')\n",
    "print(dataset_sparse.get_shape())\n",
    "# 此时的特征“矩阵”应当为一个大的字典\n",
    "print(dataset_sparse.get_shard(0)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出所有的分子，这一步得到一个94长度的列表，每个元素为1个字典，字典中为hash:{'smiles':'str', 'count':, int}\n",
    "fps_sparse = list(dataset_sparse.get_shard(0)[0])\n",
    "# print(fps_sparse)\n",
    "# print(len(fps_sparse))\n",
    "# # 这一步得到一个所有hash值的列表hash_list\n",
    "# hash_list = []\n",
    "# for i in range(94):\n",
    "#     sparse_list = list(fps_sparse[i])\n",
    "#     for j in range(len(sparse_list)):\n",
    "#         hash_list.append(sparse_list[j])\n",
    "# print(len(hash_list))\n",
    "# # 这一步得到所有特异的hash值\n",
    "# hash_list_unique = np.unique(hash_list)\n",
    "# print(len(hash_list_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique hash: 2205\n",
      "unique pieces 817\n"
     ]
    }
   ],
   "source": [
    "smiles = []\n",
    "# count = []\n",
    "for i in range(dataset_sparse.get_shard(0)[0].shape[0]):\n",
    "    # 这一步得到的依然是1个字典\n",
    "    fps_dict = fps_sparse[i]\n",
    "    for hash_value, sub_dict in fps_dict.items():\n",
    "        # 这一步把所有的smiles和count挂进列表，count会影响某种特征的权重，也有必要记录下来\n",
    "        smiles.append(sub_dict['smiles'])\n",
    "        # count.append(sub_dict['count'])\n",
    "print('unique hash:', len(smiles))\n",
    "# 这一步得到所有特异的smiles，发现1个smiles可能对应多个hash\n",
    "smiles_unique = np.unique(smiles)\n",
    "print('unique pieces', len(smiles_unique))\n",
    "# 目标是把特征矩阵缩到smiles_unique的长度，这样每一列代表一种特征，特征值即为count的权重\n",
    "# 如果这样，可以选择跳过hash，直接统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "816\n",
      "(94, 816)\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "data = np.zeros((dataset_sparse.get_shard(0)[0].shape[0], len(smiles_unique)-1))\n",
    "idx = 0\n",
    "for i in range(dataset_sparse.get_shard(0)[0].shape[0]):\n",
    "    # 这一步得到的依然是1个字典\n",
    "    fps_dict = fps_sparse[i]\n",
    "    # 数据提炼去重：\n",
    "    for hash_value, sub_dict in fps_dict.items():\n",
    "        SMILES = sub_dict['smiles']\n",
    "        COUNT = sub_dict['count']\n",
    "        # 跳过'这个空SMILES\n",
    "        if SMILES=='':\n",
    "            continue\n",
    "        if SMILES not in title:\n",
    "            title.append(SMILES)\n",
    "            data[i][idx] += COUNT\n",
    "            idx +=1\n",
    "        else:\n",
    "            data[i][title.index(SMILES)] +=COUNT\n",
    "print(idx)\n",
    "print(len(title))\n",
    "print(data.shape)\n",
    "data = data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After delete sparse coloumns: length of title: 163\n",
      "After delete sparse coloumns: shape of data: (94, 163)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "# 对于过于稀疏的特征列，设置FEATURE_RESERVE特征保留阈值进行筛选，列填充比例大于此的才会被保留\n",
    "# 同时还要保留含有特定基团的片段，此操作的优先级高于按保留阈值筛选\n",
    "delete = []\n",
    "for i in range(data.shape[1]):\n",
    "    filled_rate = sum(data[:, i]!=0)/data.shape[0]\n",
    "    if filled_rate<FEATURE_RESERVE:\n",
    "        if (EXACT_PIECE_RESERVE):                               # 这一块是在挑出含有指定集团的片段\n",
    "            m = Chem.MolFromSmarts(title[i])\n",
    "            for patt_temp in SMILES_PATT:\n",
    "                patt = Chem.MolFromSmarts(patt_temp)\n",
    "                flag = m.HasSubstructMatch(patt)\n",
    "                if flag:\n",
    "                    break\n",
    "            if flag and (filled_rate>=EXACT_PIECE_RESERVE_THRESHOLD):\n",
    "                continue\n",
    "        delete.append(i)\n",
    "data = np.delete(data, delete, axis=1)\n",
    "title = np.array(title).reshape(1, len(title))\n",
    "title = np.delete(title, delete, axis=1)\n",
    "title = title.flatten().tolist()\n",
    "print('After delete sparse coloumns: length of title:', len(title))\n",
    "print('After delete sparse coloumns: shape of data:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After atom-count control: length of title: 163\n",
      "After atom-count control: shape of data: (94, 163)\n"
     ]
    }
   ],
   "source": [
    "# 原子数量调控模块：\n",
    "if ATOM_COUNT_CONTROL:\n",
    "    delete = []\n",
    "    for i in range(data.shape[1]):\n",
    "        flag = False\n",
    "        for patt_temp in ATOM_COUNT_CONTROL_OMIT_PATTERN:\n",
    "            patt = Chem.MolFromSmarts(patt_temp)\n",
    "            flag = m.HasSubstructMatch(patt)\n",
    "            if flag:\n",
    "                break\n",
    "        if flag:\n",
    "            continue\n",
    "        m = Chem.MolFromSmarts(title[i])\n",
    "        atom_count = len(m.GetAtoms())\n",
    "        if atom_count<SMARTS_MIN_LENGTH or atom_count>SMARTS_MAX_LENGTH:\n",
    "            delete.append(i)\n",
    "    data = np.delete(data, delete, axis=1)\n",
    "    title = np.array(title).reshape(1, len(title))\n",
    "    title = np.delete(title, delete, axis=1)\n",
    "    title = title.flatten().tolist()\n",
    "    print('After atom-count control: length of title:', len(title))\n",
    "    print('After atom-count control: shape of data:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这一块里，会删除某一指定的特征列，将其转化为表示含有0,1,...,n个基团的布尔表示列，其中n为所有样本中含基团的最大值，最后会自动把空列删除\n",
    "if TURN_TO_BOOL:\n",
    "    if ALL_TURN_TO_BOOL:\n",
    "        BOOL_PATT_LIST = title\n",
    "    for bool_patt in BOOL_PATT_LIST:\n",
    "        print('Processing feature: ', bool_patt)\n",
    "        print('title length(before turn to bool):', len(title))\n",
    "        print('data shape(before turn to bool):', data.shape)\n",
    "        index_t = title.index(bool_patt)\n",
    "        print('Target conlumn index:', index_t)\n",
    "        feature_max = int(max(data[:, index_t]))\n",
    "        print('max number of target patt:', feature_max)\n",
    "        # 制作新的bool数据数组\n",
    "        if BOOL_TURN_MODE=='a':\n",
    "            matrix_bool = np.zeros((data.shape[0], feature_max))\n",
    "        else:\n",
    "            matrix_bool = np.zeros((data.shape[0], feature_max+1))\n",
    "        for i in range(data.shape[0]):\n",
    "            if BOOL_TURN_MODE=='a':\n",
    "                if data[i, index_t]>=1:\n",
    "                    matrix_bool[i, int(data[i, index_t]-1)] = 1\n",
    "            else:\n",
    "                matrix_bool[i, int(data[i, index_t])] = 1\n",
    "        # 删除对应的数据列和标题列\n",
    "        data = np.delete(data, [index_t], axis=1)\n",
    "        title = np.array(title).reshape(1, len(title))\n",
    "        title = np.delete(title, [index_t], axis=1)\n",
    "        title = title.flatten().tolist()\n",
    "        # 制作额外的标题\n",
    "        new_title = []\n",
    "        if BOOL_TURN_MODE=='a':\n",
    "            for i in range(feature_max):\n",
    "                new_title.append('With_'+str(i+1)+'_'+bool_patt)\n",
    "        else:\n",
    "            for i in range(feature_max+1):\n",
    "                new_title.append('With_'+str(i)+'_'+bool_patt)\n",
    "        data = np.hstack((data, matrix_bool))\n",
    "        title = title+new_title\n",
    "        print('title length(after turn to bool):', len(title))\n",
    "        print('data shape(after turn to bool):', data.shape, '\\n')\n",
    "    # 将空列自动删除，V5.1加入\n",
    "    delete = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if sum(data[:, i])==0:\n",
    "            delete.append(i)\n",
    "    print(data.shape)\n",
    "    print(len(title))\n",
    "    data = np.delete(data, delete, axis=1)\n",
    "    title = np.array(title).reshape(1, len(title))\n",
    "    title = np.delete(title, delete, axis=1)\n",
    "    title = title.flatten().tolist()\n",
    "    print(data.shape)\n",
    "    print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 163)\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "# 数据集的整理\n",
    "print(data.shape)\n",
    "print(len(title))\n",
    "X = data\n",
    "y = dataset_sparse.get_shard(0)[1]\n",
    "title = np.array(title).reshape(X.shape[1], )\n",
    "smiles_out = dataset_sparse.get_shard(0)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 如果需要包含描述符，这会在这里进行计算和矩阵的合并\n",
    "if INCLUDE_DESC:\n",
    "    from rdkit.Chem import Descriptors\n",
    "    loader_desc = dc.data.CSVLoader(\n",
    "        tasks=[dataset.columns.values[1]], smiles_field=dataset.columns.values[0],\n",
    "        featurizer=featurizer_desc)\n",
    "    dataset_desc = loader_desc.featurize(dataset_file)\n",
    "    print('Desc:')\n",
    "    Desc_list = dc.feat.RDKitDescriptors.allowedDescriptors\n",
    "    print(dataset_desc.get_shard(0)[0].shape)\n",
    "    allow = []\n",
    "    for i in Descriptors.descList:\n",
    "        if i[0] in Desc_list:\n",
    "            allow.append(i[0])\n",
    "    print(len(allow))\n",
    "    X_desc = dataset_desc.get_shard(0)[0]\n",
    "    Desc_list = np.array(allow).reshape(1, len(allow))\n",
    "    delete = []\n",
    "    for i in range(X_desc.shape[1]):\n",
    "        if max(X_desc[:, i])==min(X_desc[:, i]):\n",
    "            delete.append(i)\n",
    "    X_desc = np.delete(X_desc, delete, axis=1)\n",
    "    Desc_list = np.delete(Desc_list, delete, axis=1).flatten().tolist()\n",
    "    X = np.hstack((X, X_desc))\n",
    "    title = title.flatten().tolist()\n",
    "    title = title+Desc_list\n",
    "    title = np.array(title).reshape(len(title), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个目录来保存生成的数据包\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "if INCLUDE_DESC:\n",
    "    DIR = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())+'_'+str(X.shape[0])+'_'+str(data.shape[1])+'+'+str(X_desc.shape[1])\n",
    "else:\n",
    "    DIR = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())+'_'+str(X.shape[0])+'_'+str(X.shape[1])\n",
    "os.mkdir(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163,) (94, 163)\n"
     ]
    }
   ],
   "source": [
    "print(title.shape, X.shape)\n",
    "# 保存一个带Title但是不带标签值的数据包\n",
    "out = np.vstack((title, X))\n",
    "if INCLUDE_DESC:\n",
    "    OUT_NAME0 = 'Data_'+str(data.shape[0])+'_'+str(data.shape[1])+'+'+str(X_desc.shape[1])+'_withTitle_Nolabel.csv'\n",
    "else:\n",
    "    OUT_NAME0 = 'Data_'+str(data.shape[0])+'_'+str(data.shape[1])+'_withTitle_Nolabel.csv'\n",
    "dir_out0 = Path('.', DIR, OUT_NAME0)\n",
    "np.savetxt(dir_out0, out, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (94, 163)     y:  (94, 1)     title: (163,)     smiles_list: (94,)\n"
     ]
    }
   ],
   "source": [
    "# 输出其它数据包\n",
    "print('X: ', X.shape, '    y: ', y.shape, '    title:', title.shape, '    smiles_list:', smiles_out.shape)\n",
    "if INCLUDE_DESC:\n",
    "    OUT_NAME1 = 'Features_'+str(data.shape[0])+'_'+str(data.shape[1])+'+'+str(X_desc.shape[1])+'.csv'\n",
    "else:\n",
    "    OUT_NAME1 = 'Features_'+str(data.shape[0])+'_'+str(data.shape[1])+'.csv'\n",
    "dir_out1 = Path('.', DIR, OUT_NAME1)\n",
    "OUT_NAME2 = 'Values_'+str(data.shape[0])+'.csv'\n",
    "dir_out2 = Path('.', DIR, OUT_NAME2)\n",
    "if INCLUDE_DESC:\n",
    "    OUT_NAME3 = 'Title_'+str(data.shape[1])+'+'+str(X_desc.shape[1])+'.csv'\n",
    "else:\n",
    "    OUT_NAME3 = 'Title_'+str(data.shape[1])+'.csv'\n",
    "dir_out3 = Path('.', DIR, OUT_NAME3)\n",
    "OUT_NAME4 = 'SMILES_'+str(data.shape[0])+'.csv'\n",
    "dir_out4 = Path('.', DIR, OUT_NAME4)\n",
    "np.savetxt(dir_out1, X, fmt='%s', delimiter=',')\n",
    "np.savetxt(dir_out2, y, fmt='%s')\n",
    "np.savetxt(dir_out3, title, fmt='%s')\n",
    "np.savetxt(dir_out4, smiles_out, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 163) (94, 1) (164,)\n",
      "(95, 164)\n"
     ]
    }
   ],
   "source": [
    "# 输出一个带Title和Label的无SMILES完整数据集\n",
    "title = title.flatten().tolist()\n",
    "title = title+['value']\n",
    "title = np.array(title).reshape(len(title), )\n",
    "print(X.shape, y.shape, title.shape)\n",
    "out = np.vstack((title, np.hstack((X, y))))\n",
    "print(out.shape)\n",
    "if INCLUDE_DESC:\n",
    "    OUT_NAME5 = 'Data_'+str(data.shape[0])+'_'+str(data.shape[1])+'+'+str(X_desc.shape[1])+'_withTitle_withLabel.csv'\n",
    "else:\n",
    "    OUT_NAME5 = 'Data_'+str(data.shape[0])+'_'+str(data.shape[1])+'_withTitle_withLabel.csv'\n",
    "dir_out5 = Path('.', DIR, OUT_NAME5)\n",
    "np.savetxt(dir_out5, out, fmt='%s', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonForDYH",
   "language": "python",
   "name": "dyhpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
